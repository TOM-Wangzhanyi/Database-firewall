{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from sklearn import tree\n",
    "from tensorflow.keras import models, layers\n",
    "import warnings\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report, f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve,precision_recall_fscore_support\n",
    "import seaborn as sns\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import sqlparse "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape: (4200, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a'</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a' --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a' or 1 = 1; --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>' and 1 = 0 )  union all</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>? or 1 = 1 --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>x' and userid is NULL; --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>x' and email is NULL; --</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Sentence  Label\n",
       "0                          a      1\n",
       "1                        a'       1\n",
       "2                      a' --      1\n",
       "3            a' or 1 = 1; --      1\n",
       "4                          @      1\n",
       "5                          ?      1\n",
       "6   ' and 1 = 0 )  union all      1\n",
       "7              ? or 1 = 1 --      1\n",
       "8  x' and userid is NULL; --      1\n",
       "9   x' and email is NULL; --      1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './datasets/'\n",
    "df = pd.read_csv(path + \"sqli.csv\",encoding='utf-16')\n",
    "print(\"Data Shape:\", df.shape)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sql(sql):\n",
    "    try:\n",
    "        stmt = sqlparse.parse(sql)[0]\n",
    "    except:\n",
    "        stmt = None\n",
    "    if stmt == None:\n",
    "        return None\n",
    "    else :\n",
    "        return stmt.tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4200,) (4200,)\n",
      "Dataset Input: \n",
      " 0                                  [a]\n",
      "1                            [a, ',  ]\n",
      "2                        [a, ',  , --]\n",
      "3    [a, ',  , or,  , 1 = 1, ;,  , --]\n",
      "4                                  [@]\n",
      "Name: Sentence, dtype: object\n",
      "Dataset Label: \n",
      " 0    1\n",
      "1    1\n",
      "2    1\n",
      "3    1\n",
      "4    1\n",
      "Name: Label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X = df['Sentence']\n",
    "y = df['Label']\n",
    "print(X.shape, y.shape)\n",
    "X = X.apply(parse_sql) \n",
    "\n",
    "print(\"Dataset Input:\", \"\\n\", X.head(5))\n",
    "print(\"Dataset Label:\", \"\\n\", y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "\n",
    "\n",
    "mymodel = tf.keras.models.load_model('my_model_cnn_Modified.h5')\n",
    "myvectorizer = pickle.load(open(\"vectorizer_cnn_sqlparse\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(input_val):\n",
    "\n",
    "    input_val=input_val.replace('\\n', '')\n",
    "    input_val=input_val.replace('%20', ' ')\n",
    "    input_val=input_val.replace('=', ' = ')\n",
    "    input_val=input_val.replace('((', ' (( ')\n",
    "    input_val=input_val.replace('))', ' )) ')\n",
    "    input_val=input_val.replace('(', ' ( ')\n",
    "    input_val=input_val.replace(')', ' ) ')\n",
    "    input_val=input_val.replace('1 ', 'numeric')\n",
    "    input_val=input_val.replace(' 1', 'numeric')\n",
    "    input_val=input_val.replace(\"'1 \", \"'numeric \")\n",
    "    input_val=input_val.replace(\" 1'\", \" numeric'\")\n",
    "    input_val=input_val.replace('1,', 'numeric,')\n",
    "    input_val=input_val.replace(\" 2 \", \" numeric \")\n",
    "    input_val=input_val.replace(' 3 ', ' numeric ')\n",
    "    input_val=input_val.replace(' 3--', ' numeric--')\n",
    "    input_val=input_val.replace(\" 4 \", ' numeric ')\n",
    "    input_val=input_val.replace(\" 5 \", ' numeric ')\n",
    "    input_val=input_val.replace(' 6 ', ' numeric ')\n",
    "    input_val=input_val.replace(\" 7 \", ' numeric ')\n",
    "    input_val=input_val.replace(\" 8 \", ' numeric ')\n",
    "    input_val=input_val.replace('1234', ' numeric ')\n",
    "    input_val=input_val.replace(\"22\", ' numeric ')\n",
    "    input_val=input_val.replace(\" 8 \", ' numeric ')\n",
    "    input_val=input_val.replace(\" 200 \", ' numeric ')\n",
    "    input_val=input_val.replace(\"23 \", ' numeric ')\n",
    "    input_val=input_val.replace('\"1', '\"numeric')\n",
    "    input_val=input_val.replace('1\"', '\"numeric')\n",
    "    input_val=input_val.replace(\"7659\", 'numeric')\n",
    "    input_val=input_val.replace(\" 37 \", ' numeric ')\n",
    "    input_val=input_val.replace(\" 45 \", ' numeric ')\n",
    "\n",
    "    return input_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=myvectorizer.transform(X.astype('U')).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3780, 3966)\n",
      "(3780,)\n",
      "(420, 3966)\n",
      "(420,)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_dict = {}\n",
    "precision_dict = {}\n",
    "recall_dict = {}\n",
    "accuracy_dict = {}\n",
    "train_accuracy = {}\n",
    "validation_accuracy = {}\n",
    "test_accuracy = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train.reshape(-1, 1, 3966)\n",
    "X_test1 = X_test.reshape(-1, 1, 3966)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119/119 [==============================] - 0s 1ms/step\n",
      "Accuracy of CNN on test set : 0.8597883597883598\n",
      "F1 Score of CNN on test set : 0.7820723684210527\n"
     ]
    }
   ],
   "source": [
    "y_pred = mymodel.predict(X_train1).flatten()\n",
    "# y_pred1 = [1 if x>-0.5 else 0 for x in y_pred]\n",
    "y_pred = np.round(y_pred)\n",
    "print(f\"Accuracy of CNN on test set : {accuracy_score(y_pred, y_train)}\")\n",
    "print(f\"F1 Score of CNN on test set : {f1_score(y_pred, y_train)}\")\n",
    "\n",
    "# Updates model score to f1_dict\n",
    "f1_dict[\"CNN\"] = f1_score(y_pred, y_train)\n",
    "precision_dict[\"CNN\"] = precision_score(y_pred, y_train)\n",
    "recall_dict[\"CNN\"] = recall_score(y_pred, y_train)\n",
    "accuracy_dict['CNN'] = accuracy_score(y_pred, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
